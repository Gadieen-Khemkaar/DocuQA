ğŸ“˜ Document QA Bot
A Question-Answering (QA) Bot built with LangChain that lets you query documents (PDFs, text) using LLMs such as Groq (LLaMA-3/Mixtral) or Gemini.

Ask questions to your documents and get instant, accurate answers with source references.

ğŸš€ Features

ğŸ“„ Load PDFs or text files

ğŸ”¢ Free, local embeddings via HuggingFace (all-MiniLM-L6-v2)

âš¡ Fast LLM inference with Groq or Gemini

ğŸ§  Returns answers + source references

âœ‚ï¸ Handles long documents by chunking with overlap for context preservation

ğŸ”„ Modular design: switch between LLMs or embeddings easily

âš™ï¸ Installation
# Core dependencies
pip install langchain langchain-community faiss-cpu pypdf sentence-transformers

# Optional: Groq support
pip install langchain-groq

# Optional: Gemini (Google Generative AI) support
pip install langchain-google-genai

ğŸ› ï¸ Usage Example
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq  # or ChatGoogleGenerativeAI

# 1ï¸âƒ£ Load document
loader = PyPDFLoader("sample.pdf")
documents = loader.load()

# 2ï¸âƒ£ Split into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(documents)

# 3ï¸âƒ£ Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4ï¸âƒ£ Store in vector database
vectorstore = FAISS.from_documents(docs, embeddings)

# 5ï¸âƒ£ Setup retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# 6ï¸âƒ£ Initialize LLM
llm = ChatGroq(model="llama3-8b-8192", temperature=0)

# 7ï¸âƒ£ Build QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 8ï¸âƒ£ Ask a question
query = "What are the main findings of the document?"
response = qa_chain.invoke({"query": query})

print("Answer:", response["result"])
print("Sources:", [doc.metadata for doc in response["source_documents"]])

ğŸ” Code Breakdown
Component	Description
Document Loader	PyPDFLoader extracts text & metadata from PDFs
Text Splitter	RecursiveCharacterTextSplitter breaks text into chunks with overlap
Embeddings	HuggingFaceEmbeddings converts text into numerical vectors
Vector Store	FAISS stores embeddings for fast similarity search
Retriever	Fetches top-k relevant chunks for a query
LLM	ChatGroq or ChatGoogleGenerativeAI generates answers
QA Chain	RetrievalQA connects retriever + LLM and returns answer + sources
ğŸ”„ Workflow Diagram
User Query
     â”‚
     â–¼
  Retriever (FAISS)
     â”‚
     â–¼
Top-k Relevant Chunks
     â”‚
     â–¼
     LLM (Groq / Gemini)
     â”‚
     â–¼
Answer + Source Documents

ğŸ’¡ Example Output
Query: "What are the main findings of the document?"

Answer: The document concludes that AI improves diagnostic accuracy, reduces operational costs, 
and enhances patient outcomes, but faces challenges in data privacy and regulatory approval.

Sources: [{'page': 5}, {'page': 12}]

âš ï¸ Notes

Switching LLMs: Replace ChatGroq with ChatGoogleGenerativeAI for Gemini.

Embeddings: You can use other HuggingFace models if needed.

Chunk size & overlap: Adjust depending on document size and LLM context window.

FAISS: Can persist to disk for large datasets using vectorstore.save_local()

