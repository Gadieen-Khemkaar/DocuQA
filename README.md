# üìò Document QA Bot

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![LangChain](https://img.shields.io/badge/LangChain-v0.1.0-orange)


A **Question-Answering (QA) Bot** built with **LangChain** that lets you query documents (PDFs, text) using **LLMs** such as **Groq (LLaMA-3/Mixtral)** or **Gemini**.

> Ask questions to your documents and get **instant, accurate answers with source references**.

---

## üöÄ Features

* üìÑ Load PDFs or text files
* üî¢ Free, local embeddings via HuggingFace (`all-MiniLM-L6-v2`)
* ‚ö° Fast LLM inference with Groq or Gemini
* üß† Returns **answers + source references**
* ‚úÇÔ∏è Handles long documents by chunking with overlap for context preservation
* üîÑ Modular design: switch between LLMs or embeddings easily

---

## ‚öôÔ∏è Installation

```bash
# Core dependencies
pip install langchain langchain-community faiss-cpu pypdf sentence-transformers

# Optional: Groq support
pip install langchain-groq

# Optional: Gemini (Google Generative AI) support
pip install langchain-google-genai
```

---

## üõ†Ô∏è Usage Example

```python
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq  # or ChatGoogleGenerativeAI

# 1Ô∏è‚É£ Load document
loader = PyPDFLoader("sample.pdf")
documents = loader.load()

# 2Ô∏è‚É£ Split into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.split_documents(documents)

# 3Ô∏è‚É£ Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4Ô∏è‚É£ Store in vector database
vectorstore = FAISS.from_documents(docs, embeddings)

# 5Ô∏è‚É£ Setup retriever
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# 6Ô∏è‚É£ Initialize LLM
llm = ChatGroq(model="llama3-8b-8192", temperature=0)

# 7Ô∏è‚É£ Build QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 8Ô∏è‚É£ Ask a question
query = "What are the main findings of the document?"
response = qa_chain.invoke({"query": query})

print("Answer:", response["result"])
print("Sources:", [doc.metadata for doc in response["source_documents"]])
```

---

## üîé Code Breakdown

| Component           | Description                                                           |
| ------------------- | --------------------------------------------------------------------- |
| **Document Loader** | `PyPDFLoader` extracts text & metadata from PDFs                      |
| **Text Splitter**   | `RecursiveCharacterTextSplitter` breaks text into chunks with overlap |
| **Embeddings**      | `HuggingFaceEmbeddings` converts text into numerical vectors          |
| **Vector Store**    | `FAISS` stores embeddings for fast similarity search                  |
| **Retriever**       | Fetches top-k relevant chunks for a query                             |
| **LLM**             | `ChatGroq` or `ChatGoogleGenerativeAI` generates answers              |
| **QA Chain**        | `RetrievalQA` connects retriever + LLM and returns answer + sources   |

---

## üîÑ Workflow Diagram

```
User Query
     ‚îÇ
     ‚ñº
  Retriever (FAISS)
     ‚îÇ
     ‚ñº
Top-k Relevant Chunks
     ‚îÇ
     ‚ñº
     LLM (Groq / Gemini)
     ‚îÇ
     ‚ñº
Answer + Source Documents
```

---

## üí° Example Output

```
Query: "What are the main findings of the document?"

Answer: The document concludes that AI improves diagnostic accuracy, reduces operational costs, 
and enhances patient outcomes, but faces challenges in data privacy and regulatory approval.

Sources: [{'page': 5}, {'page': 12}]
```

---

## ‚ö†Ô∏è Notes

* **Switching LLMs**: Replace `ChatGroq` with `ChatGoogleGenerativeAI` for Gemini.
* **Embeddings**: You can use other HuggingFace models if needed.
* **Chunk size & overlap**: Adjust depending on document size and LLM context window.
* **FAISS**: Can persist to disk for large datasets using `vectorstore.save_local()`

---

## üìö References

* [LangChain Documentation](https://www.langchain.com/docs/)
* [HuggingFace Sentence Transformers](https://www.sbert.net/)
* [FAISS Vector Database](https://github.com/facebookresearch/faiss)

